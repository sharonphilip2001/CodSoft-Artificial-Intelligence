# -*- coding: utf-8 -*-
"""task 3 Image Captioning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1htPzxiGFxpnIqzuFVDB2S1So9buJoMCZ
"""

!pip install tensorflow pillow numpy gradio

import os
import string
import numpy as np
import pickle
from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add
from tensorflow.keras.models import Model

import tensorflow as tf
import gradio as gr
import random
from datetime import datetime

# STEP 2: Download Flickr8k Dataset
!wget -nc https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip
!wget -nc https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip

# Add -o option to unzip to ensure output directory is clean and predictable
!unzip -o Flickr8k_Dataset.zip
!unzip -o Flickr8k_text.zip

# STEP 3: Load Captions
def load_captions(filename):
    captions = {}
    with open(filename, 'r') as file:
        for line in file:
            tokens = line.strip().split('\t')
            # Handle potential issues with caption format not having #
            img_id_parts = tokens[0].split('#')
            img_id = img_id_parts[0]
            caption = tokens[1]
            if img_id not in captions:
                captions[img_id] = []
            # Clean the caption and add start/end tokens
            cleaned_caption = caption.lower().translate(str.maketrans('', '', string.punctuation))
            captions[img_id].append('startseq ' + cleaned_caption + ' endseq')
    return captions

# Check if the token file exists before loading captions
caption_file_path = "Flickr8k.token.txt"
if os.path.exists(caption_file_path):
    captions = load_captions(caption_file_path)
    print(f"Loaded {len(captions)} image captions.")
else:
    print(f"Error: Caption file not found at {caption_file_path}.")
    # Handle this error, perhaps by exiting or raising an exception
    # exit()


# STEP 4: Extract Features from Images
model_resnet = ResNet50(weights='imagenet')
model_resnet = Model(model_resnet.input, model_resnet.layers[-2].output)

def extract_features(directory):
    features = {}
    # Check if the directory exists before listing its contents
    if not os.path.isdir(directory):
        print(f"Error: Directory not found: {directory}")
        print("Checking directory structure after unzipping:")
        !ls -l
        return None # Return None or raise an error to indicate failure

    print(f"Extracting features from images in: {directory}")
    for img_name in os.listdir(directory):
        if img_name.endswith('.jpg'):
            img_path = os.path.join(directory, img_name)
            try:
                img = image.load_img(img_path, target_size=(224, 224))
                img_array = image.img_to_array(img)
                img_array = np.expand_dims(img_array, axis=0)
                img_array = preprocess_input(img_array)
                feature = model_resnet.predict(img_array, verbose=0)
                features[img_name] = feature
            except Exception as e:
                print(f"Error processing image {img_path}: {e}")
                continue # Skip this image and continue with the next
    print(f"Extracted features for {len(features)} images.")
    return features

# Check if captions were loaded successfully before extracting features
features = None
if 'captions' in globals() and captions:
    # Check if the directory exists before calling extract_features
    image_directory = "Flickr8k_Dataset" # Assuming this is the correct unzipped directory name
    if os.path.isdir(image_directory):
        features = extract_features(image_directory)
        if features is not None:
             pickle.dump(features, open("features.pkl", "wb"))
        else:
             print("Feature extraction failed.")
    else:
        print(f"Image directory '{image_directory}' not found. Please check the unzipped directory name.")
        # You might want to inspect the output of !ls -l above to find the correct directory name


# STEP 5: Create Tokenizer
tokenizer = None
vocab_size = 0
# Only proceed if captions were loaded
if 'all_captions' in globals() or ('captions' in globals() and captions):
    # Collect all captions again to be sure
    all_captions = []
    if 'captions' in globals():
        for cap_list in captions.values():
            all_captions.extend(cap_list)

    if all_captions:
        tokenizer = Tokenizer()
        tokenizer.fit_on_texts(all_captions)
        pickle.dump(tokenizer, open("tokenizer.pkl", "wb"))
        vocab_size = len(tokenizer.word_index) + 1
        print(f"Tokenizer created with vocabulary size: {vocab_size}")
    else:
        print("No captions available to create tokenizer.")
else:
    print("Skipping tokenizer creation as captions were not loaded.")


# STEP 6: Prepare Data
X1, X2, y = None, None, None
max_length = 0
# Only proceed if features, captions, and tokenizer are available
if features is not None and 'captions' in globals() and captions and tokenizer is not None:
    # Determine max length from loaded captions
    max_length = max(len(c.split()) for cap_list in captions.values() for c in cap_list)
    print(f"Maximum caption length: {max_length}")

    # Filter features to only include images that have captions
    photos_with_captions = {k: features[k] for k in captions.keys() if k in features}
    print(f"Using features for {len(photos_with_captions)} images that have captions.")


    if photos_with_captions:
         try:
            X1, X2, y = create_sequences(tokenizer, max_length, captions, photos_with_captions, vocab_size)
            print(f"Created {len(y)} training sequences.")
         except Exception as e:
             print(f"Error creating sequences: {e}")
             X1, X2, y = None, None, None
    else:
        print("No images with features and captions found to create sequences.")
else:
    print("Skipping data preparation: missing features, captions, or tokenizer.")


# STEP 7: Define the Model
model = None
# Only define and compile the model if data is prepared and vocab_size is known
if X1 is not None and X2 is not None and y is not None and vocab_size > 0 and max_length > 0:
    inputs1 = Input(shape=(2048,))
    fe1 = Dropout(0.5)(inputs1)
    fe2 = Dense(256, activation='relu')(fe1)

    inputs2 = Input(shape=(max_length,))
    # Ensure vocab_size is correct and embedding dimension matches Dense layer
    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)
    se2 = Dropout(0.5)(se1)
    se3 = LSTM(256)(se2)

    decoder1 = add([fe2, se3])
    decoder2 = Dense(256, activation='relu')(decoder1)
    outputs = Dense(vocab_size, activation='softmax')(decoder2)

    model = Model(inputs=[inputs1, inputs2], outputs=outputs)
    model.compile(loss='categorical_crossentropy', optimizer='adam')
    print("Model defined and compiled.")
else:
    print("Skipping model definition and compilation: data not prepared or vocab_size/max_length not set.")


# STEP 8: Train the Model
# Only train if the model and data are available
if model is not None and X1 is not None and X2 is not None and y is not None:
    print("Starting model training...")
    # Add validation split if desired, or train on all data for now
    model.fit([X1, X2], y, epochs=10, batch_size=64)
    model.save("image_caption_model.h5")
    print("Model trained and saved.")
else:
    print("Skipping model training: model or data not available.")


# STEP 9: Generate Caption (Keep the function as is)
def generate_caption(photo, model, tokenizer, max_length):
    in_text = 'startseq'
    # Ensure model and tokenizer are not None
    if model is None or tokenizer is None:
        print("Error: Model or Tokenizer not available for caption generation.")
        return "Caption generation failed."

    for _ in range(max_length):
        sequence = tokenizer.texts_to_sequences([in_text])[0]
        sequence = pad_sequences([sequence], maxlen=max_length)
        try:
            yhat = model.predict([photo, sequence], verbose=0)
            yhat = np.argmax(yhat)
        except Exception as e:
            print(f"Error during model prediction: {e}")
            return "Caption generation failed due to prediction error."

        # Use .get() with a default value to avoid KeyError
        word = tokenizer.index_word.get(yhat, None)

        if word is None or word == 'endseq':
            break
        in_text += ' ' + word
    return in_text.replace('startseq', '').strip()


# STEP 10: Upload and Test Your Own Image
# Only allow image upload and testing if the model is trained and components exist
if model is not None and tokenizer is not None and model_resnet is not None and max_length > 0:
    print("\nSTEP 10: Upload and Test Your Own Image")
    print("Please upload an image:")
    try:
        uploaded = files.upload()
        if uploaded:
            img_path = list(uploaded.keys())[0]
            print(f"Processing uploaded image: {img_path}")
            try:
                img = image.load_img(img_path, target_size=(224, 224))
                x = image.img_to_array(img)
                x = np.expand_dims(x, axis=0)
                x = preprocess_input(x)
                # Ensure model_resnet is used correctly for feature extraction
                photo_feature = model_resnet.predict(x, verbose=0) # Use the feature extraction model

                caption = generate_caption(photo_feature, model, tokenizer, max_length) # Use the image captioning model and extracted feature
                print("Generated Caption:", caption)
            except Exception as e:
                print(f"Error processing uploaded image or generating caption: {e}")
        else:
            print("No file uploaded.")
    except Exception as e:
        print(f"Error during file upload: {e}")
else:
    print("\nSkipping image upload and testing: Model, tokenizer, or other components are not ready.")

!pip install transformers

from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer
import torch
from PIL import Image

model = VisionEncoderDecoderModel.from_pretrained('nlpconnect/vit-gpt2-image-captioning')

feature_extractor= ViTFeatureExtractor.from_pretrained('nlpconnect/vit-gpt2-image-captioning')
tokenizer= AutoTokenizer.from_pretrained('nlpconnect/vit-gpt2-image-captioning')

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

max_length = 16
num_beams = 4
gen_kwargs= {"max_length":max_length, "num_beams":num_beams}

def predict_step(image_paths):
  images=[]
  for image_path in image_paths:
    i_image = Image.open(image_path)
    if i_image.mode !="RGB":
      i_image = i_image.convert(mode="RGB")
    images.append(i_image)
  pixel_values = feature_extractor(images=images, return_tensors='pt').pixel_values
  pixel_values = pixel_values.to(device)

  output_ids = model.generate(pixel_values, **gen_kwargs)

  preds = tokenizer.batch_decode(output_ids, skip_special_tokens = True)
  preds = [pred.strip() for pred in preds]

  return preds

from IPython.display import display

img = Image.open("/content/Flicker8k_Dataset/2097489021_ca1b9f5c3b.jpg")
display(img)

# Modify gen_kwargs to use greedy decoding instead of beam search
max_length = 16
num_beams = 1  # Change num_beams to 1 for greedy decoding
gen_kwargs= {"max_length":max_length, "num_beams":num_beams, "do_sample":False} # Add do_sample=False

predict_step(['/content/Flicker8k_Dataset/2097489021_ca1b9f5c3b.jpg'])